# Efficient-Parameter-Finetuning-Pipeline-for-Apertus-LLM-ETH-Large-Scale-AI-Engineering-HS2025-
Efficient-Parameter Finetuning Pipeline Project contributing to the Aperture LLM as part of the ETH course Large-Scale AI Engineering HS2025.


**Necessary Steps:**

- [ ] Dataset -> Domain
- [ ] Tokenize using Apertus tokenizer
- [ ] Plug LoRA-like Adapter
- [ ] Finetune
- [ ] Benchmark

---

**Action Steps:**

- [ ] Each person suggests 3 datasets with clear idea of task for benchmarking/evaluation
- [ ] *Take a look at how the LoRA-based FT works.*

**Timeline:**
**Week 1 (till 30.11.)**
- [ ] Decide on the dataset and benchmarks
- [ ] Structure of the project
- [ ] Prepare it
**Week 2 (till 7.12.)**
- [ ] Actual Execution
**Week 3 (till 14.12)**
- [ ] Benchmarking
- [ ] Evaluation
**Final Push (till 19.12. == DEADLINE)**
- [ ] Write Report
- [ ] Clean Code and Repo
